{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import random\n",
    "import IPython.display as display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image Dimensions: \n",
    "\n",
    "Counter({((1400, 1050), 'drone'): 835,\n",
    "         ((1400, 1050), 'Bird+2_Blade_rotor'): 815,\n",
    "         ((1400, 1050), '2_blade_rotor'): 800,\n",
    "         ((1400, 1050), '3_short_blade_rotor'): 800,\n",
    "         ((700, 525), 'Bird'): 800,\n",
    "         ((1400, 1050), '3_long_blades_rotor'): 799})\n",
    "         \n",
    "trim(PIL): \n",
    "\n",
    "Counter({(1087, 855): 1649,\n",
    "         (1086, 855): 1045,\n",
    "         (1087, 856): 895,\n",
    "         (542, 428): 800,\n",
    "         (1086, 856): 315,\n",
    "         (1087, 857): 106,\n",
    "         (1087, 858): 35,\n",
    "         (1086, 857): 3,\n",
    "         (1086, 858): 1})\n"
   ],
   "id": "4792f5da8f5e4653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "843a23c6b81df468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "def trim(im):\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        return im.crop(bbox)\n",
    "\n",
    "data_root = pathlib.Path('/Users/jackvittori/Desktop/uav-classification/images')\n",
    "\n",
    "tot = []\n",
    "for i, path in tqdm(enumerate([str(path) for path in data_root.glob('*/*')])):\n",
    "    \n",
    "    im = Image.open(path) \n",
    "    im2 = trim(im)\n",
    "    tot.append(im2.size)\n",
    "    \n",
    "Counter(tot)"
   ],
   "id": "538c5f4a36d3f094",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:53:34.821626Z",
     "start_time": "2025-03-11T16:53:34.755283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trim_tensorflow(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Removes uniform borders from an image using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Input image tensor of shape (H, W, 3).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Cropped image tensor.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale to detect edges\n",
    "    grayscale = tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "    # Create a mask of non-zero pixels (assumes background is uniform)\n",
    "    mask = tf.math.reduce_any(grayscale < 0.99, axis=-1)  # Assuming background is near white\n",
    "\n",
    "    # Get bounding box coordinates\n",
    "    coords = tf.where(mask)\n",
    "    ymin, xmin = tf.reduce_min(coords, axis=0)[:2]\n",
    "    ymax, xmax = tf.reduce_max(coords, axis=0)[:2]\n",
    "\n",
    "    # Crop image based on detected bounding box\n",
    "    trimmed_image = tf.image.crop_to_bounding_box(image, ymin, xmin, ymax - ymin, xmax - xmin)\n",
    "    \n",
    "    return trimmed_image\n",
    "\n",
    "def load_and_preprocess_image(image_path: str, resize: Tuple[int, int] = (1050, 1400)) -> tf.Tensor:\n",
    "    \"\"\"Loads, trims, resizes, and normalizes an image using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        resize (Tuple[int, int], optional): Target size (height, width). Defaults to (1050, 1400).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed image tensor with shape (height, width, 3) and values normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    # Load and decode image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize to [0, 1]\n",
    "\n",
    "    # Trim borders\n",
    "    image = trim_tensorflow(image)\n",
    "\n",
    "    # Resize to target dimensions\n",
    "    image = tf.image.resize(image, resize)\n",
    "\n",
    "    return image\n",
    "\n",
    "for i, image_path in tqdm(enumerate([str(path) for path in data_root.glob('*/*')])):\n",
    "    processed_image = load_and_preprocess_image(image_path)\n",
    "\n",
    "    # Convert the tensor to a NumPy array\n",
    "    image_np = processed_image.numpy()\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")  # Hide axis\n",
    "    plt.show()\n",
    "    break"
   ],
   "id": "b326f9a1eb3b970d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Pack as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 54\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, image_path \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m([\u001B[38;5;28mstr\u001B[39m(path) \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m data_root\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*/*\u001B[39m\u001B[38;5;124m'\u001B[39m)])):\n\u001B[0;32m---> 54\u001B[0m     processed_image \u001B[38;5;241m=\u001B[39m \u001B[43mload_and_preprocess_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# Convert the tensor to a NumPy array\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     image_np \u001B[38;5;241m=\u001B[39m processed_image\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "Cell \u001B[0;32mIn[34], line 46\u001B[0m, in \u001B[0;36mload_and_preprocess_image\u001B[0;34m(image_path, resize)\u001B[0m\n\u001B[1;32m     43\u001B[0m image \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mconvert_image_dtype(image, tf\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# Normalize to [0, 1]\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Trim borders\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mtrim_tensorflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Resize to target dimensions\u001B[39;00m\n\u001B[1;32m     49\u001B[0m image \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mresize(image, resize)\n",
      "Cell \u001B[0;32mIn[34], line 26\u001B[0m, in \u001B[0;36mtrim_tensorflow\u001B[0;34m(image)\u001B[0m\n\u001B[1;32m     23\u001B[0m ymax, xmax \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_max(coords, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Crop image based on detected bounding box\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m trimmed_image \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrop_to_bounding_box\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mymin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxmin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mymax\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mymin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxmax\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mxmin\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trimmed_image\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uav/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uav/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5983\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   5981\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[1;32m   5982\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m-> 5983\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: cannot compute Pack as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Pack] name: stack"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_root = pathlib.Path('/Users/jackvittori/Desktop/uav-classification/images')\n",
    "\n",
    "tot = []\n",
    "for i, path in tqdm(enumerate([str(path) for path in data_root.glob('*/*')])):\n",
    "    \n",
    "    im = Image.open(path) \n",
    "    im2 = trim(im)\n",
    "    type(im2)\n",
    "    break"
   ],
   "id": "e7ed6d8b898479ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "im2.size",
   "id": "ae2114a4657ff1fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e06e3a37b21216c9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
